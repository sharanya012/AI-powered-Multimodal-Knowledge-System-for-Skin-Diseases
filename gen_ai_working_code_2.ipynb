{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharanya012/AI-powered-Multimodal-Knowledge-System-for-Skin-Diseases/blob/main/gen_ai_working_code_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUC42j8sYGpN"
      },
      "source": [
        "# RUN THIS: Second Agentic Workflow with Qlora\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n"
      ],
      "metadata": {
        "id": "EW0FZRycoa54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "kmader_skin_cancer_mnist_ham10000_path = kagglehub.dataset_download('kmader/skin-cancer-mnist-ham10000')\n",
        "\n",
        "print('Data source import complete.')\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "      x = os.path.join(dirname, filename)\n",
        "      if filename.endswith('.csv'):\n",
        "        print(x)\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfTnfnsmVp-7",
        "outputId": "2d5c5111-e3de-4781-925e-b7238ed59d3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'skin-cancer-mnist-ham10000' dataset.\n",
            "Data source import complete.\n",
            "/kaggle/input/skin-cancer-mnist-ham10000/hmnist_8_8_RGB.csv\n",
            "/kaggle/input/skin-cancer-mnist-ham10000/hmnist_28_28_RGB.csv\n",
            "/kaggle/input/skin-cancer-mnist-ham10000/hmnist_8_8_L.csv\n",
            "/kaggle/input/skin-cancer-mnist-ham10000/hmnist_28_28_L.csv\n",
            "/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers peft datasets faiss-cpu sentence-transformers langchain langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9zWA3vCsFND",
        "outputId": "cd7f0890-f687-4861-b00f-9a83d8cce7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.34-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.63-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.3.34-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.63-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed langgraph-0.3.34 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.63 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ormsgpack-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu --quiet"
      ],
      "metadata": {
        "id": "z3evr8OrsoFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet"
      ],
      "metadata": {
        "id": "FyF4Y0kYtBhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b06c08-8860-4b87-fb58-d6eb6ba6fc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/491.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain --quiet\n",
        "!pip install -U langchain-community --quiet\n",
        "!pip install -qU langgraph==0.2.45 langchain-google-genai==2.0.4"
      ],
      "metadata": {
        "id": "3UQeHorpsy9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd26da6-cd9f-45de-f9f1-2ee029800d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/2.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet-pytorch Pillow --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vEFIhabABe9",
        "outputId": "73ffa6f4-9307-4e60-f2e8-e49771bf4d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agentic workflow\n"
      ],
      "metadata": {
        "id": "6mrYfQjao0qm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK861J5kfFbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01d3ce9-a641-4622-b0df-d01731597b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel,\n",
        "    TaskType\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TinyLlama model\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Use TinyLlama chat model\n",
        "\n",
        "print(f\"Loading model: {model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKzCwWvvsikZ",
        "outputId": "406aff9c-ea9d-4b06-deaf-4b7beddd90e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image preprocessing function\n",
        "def preprocess_image(image_data):\n",
        "    \"\"\"\n",
        "    Process image data for input to the model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert base64 to image if it's in that format\n",
        "        if isinstance(image_data, str) and image_data.startswith('data:image'):\n",
        "            # Extract the actual base64 content\n",
        "            image_data = image_data.split(',')[1]\n",
        "            image = Image.open(io.BytesIO(base64.b64decode(image_data)))\n",
        "        elif isinstance(image_data, bytes):\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "        else:\n",
        "            image = Image.open(image_data)\n",
        "\n",
        "        # Apply transformations needed for your model\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        return transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "jaQh2XJW9Tq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom EfficientNet model class to match your saved model structure\n",
        "import torch.nn as nn  # Import the necessary module\n",
        "\n",
        "class CustomEfficientNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CustomEfficientNet, self).__init__()\n",
        "        # Load a pre-trained model with standard structure for feature extraction\n",
        "        try:\n",
        "            import timm\n",
        "            self.model = timm.create_model('efficientnet_b0', pretrained=False)\n",
        "            # Replace classifier with our own\n",
        "            self.model.classifier = nn.Linear(self.model.classifier.in_features, num_classes)\n",
        "        except ImportError:\n",
        "            # Fallback if timm is not available\n",
        "            import torchvision.models as models\n",
        "            self.model = models.efficientnet_b0(pretrained=False)\n",
        "            self.model.classifier = nn.Linear(self.model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "XTwpUNJNBCFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model with custom loading function\n",
        "def load_model(model_path, num_classes=10):\n",
        "    \"\"\"\n",
        "    Load the saved model using a custom loading approach\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create model instance\n",
        "        model = CustomEfficientNet(num_classes=num_classes)\n",
        "\n",
        "        # Load state dict\n",
        "        state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "        # If you still have issues with key names, you may need to map them\n",
        "        # This is a simplified example - you might need to expand this mapping\n",
        "        new_state_dict = {}\n",
        "\n",
        "        # Print first few keys to help debug\n",
        "        print(\"First few keys in the state dict:\")\n",
        "        for i, key in enumerate(list(state_dict.keys())[:5]):\n",
        "            print(f\"  {key}\")\n",
        "\n",
        "        # Option 1: Try to load with strict=False\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "        # Option 2: If that doesn't work, implement a custom mapping\n",
        "        # This would require analyzing your model's actual structure\n",
        "        # and mapping the keys appropriately\n",
        "\n",
        "        model.eval()\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"\\nAttempting fallback method...\")\n",
        "        return fallback_model(model_path, num_classes)"
      ],
      "metadata": {
        "id": "FO3EkL4k9ZX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fallback_model(model_path, num_classes=10):\n",
        "    \"\"\"\n",
        "    Fallback method for model loading\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a simpler model for image classification\n",
        "        import torchvision.models as models\n",
        "        model = models.resnet18(pretrained=False)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "        print(\"Using ResNet18 fallback model. Note: This will NOT use your trained weights.\")\n",
        "        print(\"Images will be classified using a generic model.\")\n",
        "\n",
        "        model.eval()\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating fallback model: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "yQl90nePBGiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify image\n",
        "def classify_image(model, image_tensor, class_names):\n",
        "    \"\"\"\n",
        "    Classify image using the loaded model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if model is None:\n",
        "            return [{\"class\": \"Unknown\", \"confidence\": \"N/A - Model not loaded\"}]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image_tensor)\n",
        "            confidence = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
        "\n",
        "            # Get top 3 predictions or fewer if we have fewer classes\n",
        "            top_count = min(3, len(class_names))\n",
        "            top_conf, top_preds = torch.topk(confidence, top_count)\n",
        "\n",
        "            results = []\n",
        "            for i in range(top_count):\n",
        "                class_id = top_preds[i].item()\n",
        "                class_name = class_names[class_id] if class_id < len(class_names) else f\"Class {class_id}\"\n",
        "                confidence = top_conf[i].item() * 100\n",
        "                results.append({\n",
        "                    \"class\": class_name,\n",
        "                    \"confidence\": f\"{confidence:.2f}%\"\n",
        "                })\n",
        "\n",
        "            return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error during classification: {e}\")\n",
        "        return [{\"class\": \"Error\", \"confidence\": f\"Classification failed: {str(e)}\"}]"
      ],
      "metadata": {
        "id": "5B4P6uTu9aLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EFFICIENTNET_PATH = \"./skin_cancer_model.pth\"  # Update this path\n",
        "model = load_model(EFFICIENTNET_PATH)\n",
        "\n",
        "# Define your class names\n",
        "class_names = [\n",
        "'akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLQLk3R1-3Ir",
        "outputId": "b127b085-4426-4c52-f2fc-8c0742508b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few keys in the state dict:\n",
            "  conv_stem.weight\n",
            "  bn1.weight\n",
            "  bn1.bias\n",
            "  bn1.running_mean\n",
            "  bn1.running_var\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure quantization for loading the model\n",
        "if device == \"cuda\":\n",
        "    # Use 4-bit quantization for GPU\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "else:\n",
        "    # For CPU, we'll use 8-bit quantization or standard loading\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map={\"\": device},\n",
        "           # offload_folder=\"./offload\",  # this is the fix\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading quantized model: {e}\")\n",
        "        print(\"Falling back to standard loading...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "doR0UAiZtOsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set padding token\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Create LLM inference pipeline\n",
        "# llm_pipeline = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     max_new_tokens=512,\n",
        "#     do_sample=True,\n",
        "#     temperature=0.7,\n",
        "#     top_p=0.95\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snel706wtTHv",
        "outputId": "b6e9c26d-b79a-4f7b-85db-f4cdc076a55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tinyllama_llm(prompt):\n",
        "    \"\"\"Generate responses using TinyLlama model\"\"\"\n",
        "    print(\"In original function\")\n",
        "    try:\n",
        "        # Format input for TinyLlama chat model\n",
        "        formatted_prompt = f\"<human>: {prompt}\\n<assistant>:\"\n",
        "        response = llm_pipeline(formatted_prompt)[0][\"generated_text\"]\n",
        "\n",
        "        # Extract only the assistant's response\n",
        "        assistant_response = response.split(\"<assistant>:\", 1)[1].strip()\n",
        "\n",
        "        # Clean up any potential trailing tokens\n",
        "        if \"<human>\" in assistant_response:\n",
        "            assistant_response = assistant_response.split(\"<human>\")[0].strip()\n",
        "\n",
        "        return assistant_response\n",
        "    except Exception as e:\n",
        "        print(f\"TinyLlama error: {e}\")\n",
        "        return \"I couldn't generate a response. Please try again.\"\n"
      ],
      "metadata": {
        "id": "JbLkLiA0teRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "# For demonstration, create a sample dataframe with HAM10000 structure\n",
        "print(\"Loading and preparing dataset...\")\n",
        "sample_data = {\n",
        "    'dx': ['mel', 'nv', 'bcc', 'akiec', 'vasc', 'bkl', 'df'],\n",
        "    'dx_type': ['histopathology', 'confocal', 'consensus', 'histopathology', 'follow-up', 'histopathology', 'consensus'],\n",
        "    'localization': ['scalp', 'back', 'face', 'chest', 'arm', 'leg', 'neck'],\n",
        "    'age': [45, 32, 67, 55, 40, 62, 28],\n",
        "    'sex': ['male', 'female', 'male', 'female', 'male', 'female', 'male']\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Try to load the real dataset if available\n",
        "    metadata = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n",
        "    print(\"Loaded real HAM10000 dataset\")\n",
        "except:\n",
        "    # Use sample data if real dataset is not available\n",
        "    metadata = pd.DataFrame(sample_data)\n",
        "    print(\"Using sample dataset\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VixCCv17ti1O",
        "outputId": "2164b3f8-c9e3-4b15-ce60-50e1e00f0626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing dataset...\n",
            "Loaded real HAM10000 dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a text description field\n",
        "def create_description(row):\n",
        "    return f\"Diagnosis: {row['dx']}, Type: {row['dx_type']}, Located on {row['localization']}, Age: {row['age']}, Sex: {row['sex']}\""
      ],
      "metadata": {
        "id": "ixEPOvTOtpBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata['text_data'] = metadata.apply(create_description, axis=1)\n",
        "\n",
        "# Add condition descriptions\n",
        "condition_info = {\n",
        "    'mel': 'Melanoma - a serious form of skin cancer that develops from melanocytes',\n",
        "    'nv': 'Melanocytic nevus - a common mole, a benign growth of melanocytes',\n",
        "    'bcc': 'Basal cell carcinoma - the most common form of skin cancer, rarely metastasizes',\n",
        "    'akiec': 'Actinic keratosis - a precancerous skin condition caused by sun damage',\n",
        "    'vasc': 'Vascular lesion - abnormal blood vessel formation in the skin',\n",
        "    'bkl': 'Benign keratosis - a non-cancerous growth including seborrheic keratosis',\n",
        "    'df': 'Dermatofibroma - a benign skin tumor composed of fibroblastic cells'\n",
        "}\n",
        "\n",
        "# Add detailed descriptions to the metadata\n",
        "metadata['detailed_info'] = metadata['dx'].map(lambda x: condition_info.get(x, 'Unknown condition'))\n",
        "metadata['full_text'] = metadata.apply(lambda row: f\"{row['text_data']}. {row['detailed_info']}\", axis=1)"
      ],
      "metadata": {
        "id": "llbFYLmHtxk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ✅ This loads index, docs, and internal ID mappings\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"/content/faiss_store/faiss_store\",\n",
        "    embeddings=embedding_model,\n",
        "    allow_dangerous_deserialization=True  # ✅ Add this\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsrftG2wp-vL",
        "outputId": "8e75cabc-61f3-4c4a-8246-603ea185f6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-7c8a3414eb49>:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fallback_response():\n",
        "    return \"\"\"I can provide information about skin conditions like:\n",
        "- Melanoma (mel): A serious form of skin cancer that develops from melanocytes\n",
        "- Melanocytic nevus (nv): A common mole, a benign growth of melanocytes\n",
        "- Basal cell carcinoma (bcc): The most common form of skin cancer, rarely metastasizes\n",
        "- Actinic keratosis (akiec): A precancerous skin condition caused by sun damage\n",
        "- Vascular lesion (vasc): Abnormal blood vessel formation in the skin\n",
        "- Benign keratosis (bkl): A non-cancerous growth including seborrheic keratosis\n",
        "- Dermatofibroma (df): A benign skin tumor composed of fibroblastic cells\n",
        "\n",
        "What would you like to know about these conditions?\"\"\""
      ],
      "metadata": {
        "id": "TZKljAApusfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define retrieval tool with error handling\n",
        "def retrieval_tool(query):\n",
        "    try:\n",
        "        return retriever.invoke(query)\n",
        "    except Exception as e:\n",
        "        print(f\"Retrieval error: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "CNEOD-vNuwdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define chatbot logic\n",
        "# def chatbot_with_retrieval(state):\n",
        "#     # Check if we have any messages\n",
        "#     if not state[\"messages\"]:\n",
        "#         # If there are no messages, give a welcome message\n",
        "#         welcome_msg = \"Welcome to the Medical Chatbot! I can provide information about various skin conditions and analyze dermatological data. How can I assist you today?\"\n",
        "#         return {\"messages\": [AIMessage(content=welcome_msg)]}\n",
        "\n",
        "#     # Get the latest user message\n",
        "#     user_msg = None\n",
        "#     for msg in reversed(state[\"messages\"]):\n",
        "#         if isinstance(msg, HumanMessage):\n",
        "#             user_msg = msg\n",
        "#             break\n",
        "\n",
        "#     if not user_msg:\n",
        "#         return {\"messages\": state[\"messages\"] + [AIMessage(content=\"I'm not sure I understand. Could you please ask a question about skin conditions?\")]}\n",
        "\n",
        "#     user_query = user_msg.content\n",
        "#     retrieved_docs = retrieval_tool(user_query)\n",
        "\n",
        "#     if not retrieved_docs:\n",
        "#         # Use fallback response if no documents retrieved\n",
        "#         llm_response = get_fallback_response()\n",
        "#     else:\n",
        "#         # Format the retrieved context\n",
        "#         context_parts = []\n",
        "#         for i, doc in enumerate(retrieved_docs, 1):\n",
        "#             context_parts.append(f\"Document {i}: {doc.page_content}\")\n",
        "\n",
        "#         context = \"\\n\\n\".join(context_parts)\n",
        "#         prompt = f\"\"\"\n",
        "# Based on the following medical information about skin conditions, please answer the user's query. Try to provide information about the condition based on the available data.\n",
        "# If you don't have enough information, acknowledge that and provide general guidance.\n",
        "\n",
        "# RETRIEVED INFORMATION:\n",
        "# {context}\n",
        "\n",
        "# USER QUERY: {user_query}\n",
        "\n",
        "# Your response should be informative, accurate, and empathetic. Do not include any reference numbers or document numbers in your answer.\n",
        "# \"\"\"\n",
        "#         llm_response = tinyllama_llm(prompt)\n",
        "\n",
        "#     return {\"messages\": state[\"messages\"] + [AIMessage(content=llm_response)]}\n",
        "\n",
        "# Update the chatbot function to handle images\n",
        "def chatbot_with_retrieval(state):\n",
        "    # Check if we have any messages\n",
        "    if not state[\"messages\"]:\n",
        "        # If there are no messages, give a welcome message\n",
        "        welcome_msg = \"Welcome to the Medical Chatbot! I can provide information about various skin conditions and analyze dermatological images. How can I assist you today?\"\n",
        "        return {\"messages\": [AIMessage(content=welcome_msg)]}\n",
        "\n",
        "    # Get the latest user message\n",
        "    user_msg = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            user_msg = msg\n",
        "            break\n",
        "\n",
        "    if not user_msg:\n",
        "        return {\"messages\": state[\"messages\"] + [AIMessage(content=\"I'm not sure I understand. Could you please ask a question or upload an image of a skin condition?\")]}\n",
        "\n",
        "    # Check if the message contains an image\n",
        "    has_image = False\n",
        "    image_data = None\n",
        "    user_query = user_msg.content\n",
        "\n",
        "    # Handle different message types depending on your frontend\n",
        "    if hasattr(user_msg, 'content') and isinstance(user_msg.content, list):\n",
        "        # Handle multimodal messages where content is a list of different parts\n",
        "        text_parts = []\n",
        "        for part in user_msg.content:\n",
        "            if part.get('type') == 'text':\n",
        "                text_parts.append(part.get('text', ''))\n",
        "            elif part.get('type') == 'image_url' and 'url' in part:\n",
        "                has_image = True\n",
        "                image_data = part['url']\n",
        "\n",
        "        user_query = ' '.join(text_parts)\n",
        "\n",
        "    # If the user shared an image\n",
        "    if has_image and image_data and model:\n",
        "        processed_image = preprocess_image(image_data)\n",
        "        if processed_image is not None:\n",
        "            # Classify the image\n",
        "            results = classify_image(model, processed_image, class_names)\n",
        "\n",
        "            if results:\n",
        "                # Create response with classification results\n",
        "                response = \"I've analyzed the skin condition image and here are my findings:\\n\\n\"\n",
        "                for i, result in enumerate(results, 1):\n",
        "                    response += f\"{i}. {result['class']} - {result['confidence']} confidence\\n\"\n",
        "\n",
        "                # Add some general information about the top condition\n",
        "                top_condition = results[0]['class']\n",
        "                retrieved_docs = retrieval_tool(top_condition)\n",
        "\n",
        "                if retrieved_docs:\n",
        "                    context_parts = []\n",
        "                    for i, doc in enumerate(retrieved_docs, 1):\n",
        "                        context_parts.append(f\"{doc.page_content}\")\n",
        "                    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "                    prompt = f\"\"\"\n",
        "                    Based on the image analysis, the most likely condition is {top_condition}.\n",
        "\n",
        "                    Here's relevant information about this condition:\n",
        "                    {context}\n",
        "\n",
        "                    The user query was: {user_query if user_query else \"Image analysis\"}\n",
        "\n",
        "                    Provide a helpful, informative response about this condition based on the image analysis and retrieved information.\n",
        "                    Don't mention document numbers or reference numbers.\n",
        "                    \"\"\"\n",
        "\n",
        "                    detailed_response = tinyllama_llm(prompt)\n",
        "                    response += f\"\\n\\n{detailed_response}\"\n",
        "                else:\n",
        "                    response += f\"\\n\\nI identified the top condition as {top_condition}, but I don't have detailed information about it in my knowledge base. I recommend consulting with a dermatologist for a proper diagnosis.\"\n",
        "\n",
        "                return {\"messages\": state[\"messages\"] + [AIMessage(content=response)]}\n",
        "            else:\n",
        "                return {\"messages\": state[\"messages\"] + [AIMessage(content=\"I couldn't analyze the image properly. Please make sure it's a clear image of the skin condition and try again.\")]}\n",
        "\n",
        "    # If no image or image processing failed, use the text-based approach\n",
        "    retrieved_docs = retrieval_tool(user_query)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        # Use fallback response if no documents retrieved\n",
        "        llm_response = get_fallback_response()\n",
        "    else:\n",
        "        # Format the retrieved context\n",
        "        context_parts = []\n",
        "        for i, doc in enumerate(retrieved_docs, 1):\n",
        "            context_parts.append(f\"Document {i}: {doc.page_content}\")\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following medical information about skin conditions, please answer the user's query.\n",
        "        Try to provide information about the condition based on the available data.\n",
        "        If you don't have enough information, acknowledge that and provide general guidance.\n",
        "\n",
        "        RETRIEVED INFORMATION:\n",
        "        {context}\n",
        "\n",
        "        USER QUERY:\n",
        "        {user_query}\n",
        "\n",
        "        Your response should be informative, accurate, and empathetic.\n",
        "        Do not include any reference numbers or document numbers in your answer.\n",
        "        \"\"\"\n",
        "\n",
        "        llm_response = tinyllama_llm(prompt)\n",
        "\n",
        "    return {\"messages\": state[\"messages\"] + [AIMessage(content=llm_response)]}"
      ],
      "metadata": {
        "id": "bXl6b2CRuxcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def human_input(state):\n",
        "#     # Print the last AI message\n",
        "#     for msg in reversed(state[\"messages\"]):\n",
        "#         if isinstance(msg, AIMessage):\n",
        "#             print(f\"Bot: {msg.content}\")\n",
        "#             break\n",
        "\n",
        "#     user_input = input(\"User: \")\n",
        "#     if user_input.lower() in {\"exit\", \"quit\", \"bye\"}:\n",
        "#         return {\"finished\": True, \"messages\": state[\"messages\"] + [HumanMessage(content=user_input)]}\n",
        "\n",
        "#     return {\"messages\": state[\"messages\"] + [HumanMessage(content=user_input)]}\n",
        "# Update your human input function to handle image uploads\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "import time\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from langchain.schema import HumanMessage\n",
        "import io\n",
        "import base64\n",
        "from PIL import Image\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "from google.colab import files\n",
        "import io\n",
        "from langchain.schema import HumanMessage\n",
        "import time\n",
        "\n",
        "def handle_file_upload(state):\n",
        "    \"\"\"Use Colab's built-in file uploader instead of widgets\"\"\"\n",
        "\n",
        "    print(\"Please upload an image file when prompted...\")\n",
        "\n",
        "    # Use Colab's built-in upload function\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file was uploaded\")\n",
        "        return {\"messages\": state[\"messages\"]}\n",
        "\n",
        "    # Process the first uploaded file\n",
        "    for filename, content in uploaded.items():\n",
        "        print(f\"Processing {filename} ({len(content)} bytes)...\")\n",
        "\n",
        "        # Add to messages\n",
        "        state[\"messages\"].append(HumanMessage(content=[\n",
        "            {\"type\": \"text\", \"text\": \"Please analyze this image.\"},\n",
        "            {\"type\": \"image_bytes\", \"bytes\": content}\n",
        "        ]))\n",
        "\n",
        "        print(\"✅ Image uploaded successfully!\")\n",
        "        break  # Only process the first file\n",
        "\n",
        "    return {\"messages\": state[\"messages\"]}\n",
        "\n",
        "\n",
        "def human_input(state):\n",
        "    # Print the last AI message\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, AIMessage):\n",
        "            print(f\"Bot: {msg.content}\")\n",
        "            break\n",
        "\n",
        "    # In a real frontend, you'd handle image upload differently\n",
        "    # This is a simplified CLI example\n",
        "    print(\"Options: [1] Ask a question [2] Upload an image [3] Exit\")\n",
        "    choice = input(\"Choice: \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        user_input = input(\"Your question: \")\n",
        "        return {\"messages\": state[\"messages\"] + [HumanMessage(content=user_input)]}\n",
        "    elif choice == \"2\":\n",
        "        return handle_file_upload(state)\n",
        "    elif choice == \"3\" or choice.lower() in {\"exit\", \"quit\", \"bye\"}:\n",
        "        return {\"finished\": True, \"messages\": state[\"messages\"] + [HumanMessage(content=\"bye\")]}\n",
        "    else:\n",
        "        print(\"Invalid choice, please try again.\")\n",
        "        return human_input(state)\n",
        "\n",
        "\n",
        "def maybe_exit(state):\n",
        "    # Fix: Return the actual node name \"chatbot\" if not finished\n",
        "    return END if state.get(\"finished\", False) else \"chatbot\""
      ],
      "metadata": {
        "id": "IsKbDQG5vKMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # For images\n",
        "# from langchain.agents import Tool\n",
        "\n",
        "# def classify_skin_image(image_path: str) -> str:\n",
        "#     # Call your EfficientNet model here\n",
        "#     return predict_image(image_path)  # returns label like \"mel\"\n",
        "\n",
        "# image_tool = Tool.from_function(\n",
        "#     func=classify_skin_image,\n",
        "#     name=\"classify_skin_image\",\n",
        "#     description=\"Classifies a skin image and returns the predicted condition.\"\n",
        "# )"
      ],
      "metadata": {
        "id": "ZQzuwi328Zy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def image_input_node(state):\n",
        "#     # Get the last message\n",
        "#     for msg in reversed(state[\"messages\"]):\n",
        "#         if isinstance(msg, HumanMessage):\n",
        "#             if msg.content.endswith((\".jpg\", \".jpeg\", \".png\")) and os.path.exists(msg.content):\n",
        "#                 prediction = classify_skin_image(msg.content)\n",
        "#                 return {\"messages\": state[\"messages\"] + [AIMessage(content=f\"Image diagnosed as: {prediction}\")], \"diagnosis\": prediction}\n",
        "#     return {\"messages\": state[\"messages\"] + [AIMessage(content=\"Please upload a valid image path.\")]}\n",
        "\n",
        "# graph_builder.add_node(\"classify_image\", image_input_node)\n"
      ],
      "metadata": {
        "id": "gBetqRYs8eq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Build conversation graph\n",
        "# print(\"Building conversation graph...\")\n",
        "# graph_builder = StateGraph(dict)\n",
        "# graph_builder.add_node(\"chatbot\", chatbot_with_retrieval)\n",
        "# graph_builder.add_node(\"human\", human_input)\n",
        "# graph_builder.add_edge(START, \"chatbot\")\n",
        "# graph_builder.add_edge(\"chatbot\", \"human\")\n",
        "# graph_builder.add_conditional_edges(\"human\", maybe_exit)\n",
        "# chat_graph = graph_builder.compile()\n",
        "\n",
        "print(\"Building conversation graph...\")\n",
        "graph_builder = StateGraph(dict)\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_retrieval)\n",
        "graph_builder.add_node(\"human\", human_input)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", \"human\")\n",
        "# graph_builder.add_conditional_edges(\"human\", maybe_exit)\n",
        "chat_graph = graph_builder.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i2G_PpXvO-9",
        "outputId": "82f7c986-47fe-4020-d517-e39c5d810a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building conversation graph...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA Fine-tuning function\n",
        "def fine_tune_model_with_qlora(training_data, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", output_dir=\"./finetuned_model\"):\n",
        "    \"\"\"\n",
        "    Fine-tune TinyLlama model using QLoRA\n",
        "\n",
        "    Args:\n",
        "        training_data: List of dictionaries with 'instruction' and 'response' keys\n",
        "        model_name: Base model to fine-tune\n",
        "        output_dir: Directory to save fine-tuned model\n",
        "    \"\"\"\n",
        "    print(f\"Starting QLoRA fine-tuning process for {model_name}...\")\n",
        "\n",
        "    # Format the training data\n",
        "    def format_instruction(example):\n",
        "        return f\"<human>: {example['instruction']}\\n<assistant>: {example['response']}\"\n",
        "\n",
        "    formatted_data = [format_instruction(item) for item in training_data]\n",
        "\n",
        "    # Create a dataset\n",
        "    dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Configure LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=8,  # Rank\n",
        "        lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
        "        lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Target attention modules for adaptation\n",
        "    )\n",
        "\n",
        "    # Prepare model for training\n",
        "    if device == \"cuda\":\n",
        "        # For GPU fine-tuning\n",
        "        model_to_train = prepare_model_for_kbit_training(model)\n",
        "        model_to_train = get_peft_model(model_to_train, peft_config)\n",
        "    else:\n",
        "        # For CPU fine-tuning (simpler configuration)\n",
        "        model_to_train = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=4 if device == \"cuda\" else 1,  # Smaller batch size for CPU\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=device == \"cuda\",  # Use fp16 only on GPU\n",
        "        logging_steps=10,\n",
        "        save_steps=200,\n",
        "        save_total_limit=3,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model_to_train,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    trainer.save_model(output_dir)\n",
        "\n",
        "    print(f\"Model fine-tuned and saved to {output_dir}\")\n",
        "    return output_dir\n"
      ],
      "metadata": {
        "id": "ZL81WTzsvu8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example training data for fine-tuning\n",
        "def prepare_training_data():\n",
        "    \"\"\"Prepare training data from HAM10000 dataset for fine-tuning\"\"\"\n",
        "    # Create instructions based on skin conditions\n",
        "    training_samples = []\n",
        "\n",
        "    # Create general questions about skin conditions\n",
        "    for condition, description in condition_info.items():\n",
        "        # Question about symptoms\n",
        "        training_samples.append({\n",
        "            \"instruction\": f\"What are the symptoms of {condition}?\",\n",
        "            \"response\": f\"For {condition} ({description}), typical symptoms include changes in skin appearance, \" +\n",
        "                       f\"such as color changes, irregular borders, and asymmetry. This condition is often found on \" +\n",
        "                       f\"areas like the face, neck, and sun-exposed areas. Always consult a dermatologist for diagnosis.\"\n",
        "        })\n",
        "\n",
        "        # Question about treatments\n",
        "        training_samples.append({\n",
        "            \"instruction\": f\"How is {condition} treated?\",\n",
        "            \"response\": f\"Treatment for {condition} ({description}) depends on the severity and location. \" +\n",
        "                       f\"Options may include topical medications, surgical removal, cryotherapy, or observation. \" +\n",
        "                       f\"A dermatologist should determine the best treatment approach based on individual factors.\"\n",
        "        })\n",
        "\n",
        "    # Create questions about metadata\n",
        "    for i, row in metadata.head(5).iterrows():\n",
        "        training_samples.append({\n",
        "            \"instruction\": f\"Tell me about a {row['dx']} case.\",\n",
        "            \"response\": f\"Here's information about a {row['dx']} case: {row['full_text']} \" +\n",
        "                       f\"This condition requires proper medical evaluation for diagnosis and treatment.\"\n",
        "        })\n",
        "\n",
        "    # Add general skin health questions\n",
        "    training_samples.extend([\n",
        "        {\n",
        "            \"instruction\": \"What factors increase the risk of skin cancer?\",\n",
        "            \"response\": \"Factors that increase skin cancer risk include excessive UV exposure (sun or tanning beds), \" +\n",
        "                       \"fair skin, history of sunburns, family history of skin cancer, weakened immune system, and \" +\n",
        "                       \"advanced age. Regular skin examinations and sun protection can help reduce these risks.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"How can I prevent melanoma?\",\n",
        "            \"response\": \"To prevent melanoma: limit sun exposure especially during peak hours (10am-4pm), \" +\n",
        "                       \"wear broad-spectrum sunscreen (SPF 30+), wear protective clothing, avoid tanning beds, \" +\n",
        "                       \"conduct regular skin self-exams, and see a dermatologist annually, especially if you \" +\n",
        "                       \"have risk factors like fair skin or family history.\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "    return training_samples\n"
      ],
      "metadata": {
        "id": "xqV9PaDKvvzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define function to start the chat session\n",
        "# def start_chat():\n",
        "#     print(\"Medical Chatbot Ready! Type 'exit' to quit.\")\n",
        "#     try:\n",
        "#         state = chat_graph.invoke({\"messages\": []})\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error starting chatbot: {e}\")\n",
        "#         print(\"\\nTroubleshooting tips:\")\n",
        "#         print(\"1. Check if you have properly installed all required packages\")\n",
        "#         print(\"2. Make sure you have enough memory available\")\n",
        "#         print(\"3. Try reducing model complexity or batch size\")\n",
        "# Define function to start the chat session\n",
        "def start_chat():\n",
        "    print(\"Medical Chatbot Ready! Type 'exit' to quit.\")\n",
        "    try:\n",
        "        # Enable debug mode to see what's happening\n",
        "        # set_debug(True)\n",
        "        # Initialize with empty messages and run until completion\n",
        "        state = {\"messages\": []}\n",
        "        while True:\n",
        "            # Run the graph once\n",
        "            state = chat_graph.invoke(state)\n",
        "            # Check if we're finished\n",
        "            if state.get(\"finished\", False):\n",
        "                print(\"Chat session ended.\")\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chat session: {e}\")\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"1. Check if you have properly installed all required packages\")\n",
        "        print(\"2. Make sure you have enough memory available\")\n",
        "        print(\"3. Try reducing model complexity or batch size\")\n",
        "        print(\"4. Check for compatibility issues with your model format\")"
      ],
      "metadata": {
        "id": "hHHW0X9hvzZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, clear_output, HTML\n",
        "import ipywidgets as widgets\n",
        "from google.colab import files\n",
        "import io\n",
        "import base64\n",
        "from PIL import Image\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "def create_chatbot_ui():\n",
        "    \"\"\"Create a user interface for the medical chatbot\"\"\"\n",
        "\n",
        "    # CSS for better appearance\n",
        "    display(HTML(\"\"\"\n",
        "    <style>\n",
        "    .chat-container {\n",
        "        max-height: 400px;\n",
        "        overflow-y: auto;\n",
        "        border: 1px solid #ddd;\n",
        "        padding: 10px;\n",
        "        margin-bottom: 10px;\n",
        "        background-color: #f9f9f9;\n",
        "        border-radius: 5px;\n",
        "    }\n",
        "    .user-message {\n",
        "        background-color: #e3f2fd;\n",
        "        padding: 8px 12px;\n",
        "        border-radius: 15px;\n",
        "        margin: 5px 0;\n",
        "        max-width: 80%;\n",
        "        margin-left: auto;\n",
        "        text-align: right;\n",
        "    }\n",
        "    .bot-message {\n",
        "        background-color: #f1f1f1;\n",
        "        padding: 8px 12px;\n",
        "        border-radius: 15px;\n",
        "        margin: 5px 0;\n",
        "        max-width: 80%;\n",
        "    }\n",
        "    .message-container {\n",
        "        display: flex;\n",
        "        flex-direction: column;\n",
        "        margin-bottom: 10px;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"))\n",
        "\n",
        "    # Chat container - where messages will appear\n",
        "    chat_output = widgets.Output(layout=widgets.Layout(height='400px', overflow_y='auto'))\n",
        "\n",
        "    # Text input for user messages\n",
        "    text_input = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Type your message here...',\n",
        "        description='',\n",
        "        layout=widgets.Layout(width='70%')\n",
        "    )\n",
        "\n",
        "    # Send button for text messages\n",
        "    send_button = widgets.Button(\n",
        "        description='Send',\n",
        "        button_style='primary',\n",
        "        layout=widgets.Layout(width='15%')\n",
        "    )\n",
        "\n",
        "    # Upload button for images\n",
        "    upload_button = widgets.Button(\n",
        "        description='Upload Image',\n",
        "        button_style='info',\n",
        "        layout=widgets.Layout(width='15%')\n",
        "    )\n",
        "\n",
        "    # Create layout\n",
        "    input_box = widgets.HBox([text_input, send_button, upload_button])\n",
        "\n",
        "    # Define message display functions first (before using them)\n",
        "    def print_user_message(message):\n",
        "        with chat_output:\n",
        "            display(HTML(f'<div class=\"message-container\"><div class=\"user-message\">{message}</div></div>'))\n",
        "\n",
        "    def print_bot_message(message):\n",
        "        with chat_output:\n",
        "            display(HTML(f'<div class=\"message-container\"><div class=\"bot-message\">{message}</div></div>'))\n",
        "\n",
        "    def print_image(img_data):\n",
        "        with chat_output:\n",
        "            img_b64 = base64.b64encode(img_data).decode('utf-8')\n",
        "            display(HTML(f'<div class=\"message-container\"><div class=\"user-message\"><img src=\"data:image/jpeg;base64,{img_b64}\" style=\"max-width:200px; border-radius:5px;\" /></div></div>'))\n",
        "\n",
        "    # Initialize state and display components\n",
        "    display(chat_output)\n",
        "    display(input_box)\n",
        "\n",
        "    # Initialize state with empty messages\n",
        "    state = {\"messages\": []}\n",
        "\n",
        "    # Handler for text messages\n",
        "    def handle_text_message(b=None):\n",
        "        message = text_input.value\n",
        "        if not message.strip():\n",
        "            return\n",
        "\n",
        "        # Clear input\n",
        "        text_input.value = ''\n",
        "\n",
        "        # Exit command\n",
        "        if message.lower() == 'exit':\n",
        "            print_bot_message(\"Chat session ended.\")\n",
        "            state[\"finished\"] = True\n",
        "            return\n",
        "\n",
        "        # Display user message\n",
        "        print_user_message(message)\n",
        "\n",
        "        # Add message to state\n",
        "        state[\"messages\"].append(HumanMessage(content=message))\n",
        "\n",
        "        # Process with chatbot_with_retrieval function\n",
        "        try:\n",
        "            # Call the chatbot function directly with the current state\n",
        "            new_state = chatbot_with_retrieval(state)\n",
        "\n",
        "            # Update state\n",
        "            state.update(new_state)\n",
        "\n",
        "            # Find the last AI message to display\n",
        "            for msg in reversed(state[\"messages\"]):\n",
        "                if isinstance(msg, AIMessage):\n",
        "                    print_bot_message(msg.content)\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print_bot_message(f\"Error: {str(e)}\")\n",
        "\n",
        "    # Handler for image uploads\n",
        "    def handle_image_upload(b):\n",
        "        print_bot_message(\"Please select an image of the skin condition...\")\n",
        "\n",
        "        # Use Colab's upload function\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            print_bot_message(\"No image was uploaded.\")\n",
        "            return\n",
        "\n",
        "        # Process uploaded image\n",
        "        for filename, content in uploaded.items():\n",
        "            try:\n",
        "                # Display the image in chat\n",
        "                print_image(content)\n",
        "                print_bot_message(f\"Analyzing image: {filename}...\")\n",
        "\n",
        "                # Add to messages with multi-modal content\n",
        "                state[\"messages\"].append(HumanMessage(content=[\n",
        "                    {\"type\": \"text\", \"text\": \"Please analyze this skin condition.\"},\n",
        "                    {\"type\": \"image_bytes\", \"bytes\": content}\n",
        "                ]))\n",
        "\n",
        "                # Process with chatbot function\n",
        "                new_state = chatbot_with_retrieval(state)\n",
        "\n",
        "                # Update state\n",
        "                state.update(new_state)\n",
        "\n",
        "                # Find the last AI message to display\n",
        "                for msg in reversed(state[\"messages\"]):\n",
        "                    if isinstance(msg, AIMessage):\n",
        "                        print_bot_message(msg.content)\n",
        "                        break\n",
        "\n",
        "                break  # Only process first image\n",
        "            except Exception as e:\n",
        "                print_bot_message(f\"Error processing image: {str(e)}\")\n",
        "\n",
        "    # Connect handlers to buttons\n",
        "    send_button.on_click(handle_text_message)\n",
        "    upload_button.on_click(handle_image_upload)\n",
        "\n",
        "    # Handle Enter key in text input\n",
        "    text_input.on_submit(handle_text_message)\n",
        "\n",
        "    # Show initial welcome message - moved here after print_bot_message is defined\n",
        "    print_bot_message(\"Welcome to the Medical Chatbot! How can I help you with your skin condition today?\")\n",
        "\n",
        "    # Start the chatbot by invoking it with empty messages to get welcome message\n",
        "    if not state[\"messages\"]:\n",
        "        new_state = chatbot_with_retrieval(state)\n",
        "        state.update(new_state)\n",
        "\n",
        "        # If we got a welcome message, display it\n",
        "        for msg in state[\"messages\"]:\n",
        "            if isinstance(msg, AIMessage):\n",
        "                print_bot_message(msg.content)\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        'chat_output': chat_output,\n",
        "        'input': text_input,\n",
        "        'send_button': send_button,\n",
        "        'upload_button': upload_button,\n",
        "        'state': state\n",
        "    }\n",
        "\n",
        "# Updated start_chat function that uses the UI\n",
        "def start_chat():\n",
        "    print(\"Medical Chatbot Ready!\")\n",
        "\n",
        "    try:\n",
        "        # Display UI and let it handle the interaction\n",
        "        ui = create_chatbot_ui()\n",
        "\n",
        "        # No need for the while loop from the original function\n",
        "        # as the UI buttons now handle the interaction flow\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chat session: {e}\")\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"1. Check if you have properly installed all required packages\")\n",
        "        print(\"2. Make sure you have enough memory available\")\n",
        "        print(\"3. Try reducing model complexity or batch size\")\n",
        "        print(\"4. Check for compatibility issues with your model format\")"
      ],
      "metadata": {
        "id": "5vJf7FNNPVJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you're using the finetuned_model folder then run this (unzip the folder first)\n"
      ],
      "metadata": {
        "id": "b_ZVB2ziBEjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./offload"
      ],
      "metadata": {
        "id": "QpfURV2T90ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    fine_tuned_dir = \"/content/drive/MyDrive/finetuned_model\"\n",
        "\n",
        "    # Load EfficientNet model for image classification\n",
        "    EFFICIENTNET_PATH = \"/content/drive/MyDrive/skin_cancer_model.pth\"  # Update this path\n",
        "    try:\n",
        "        print(f\"Loading image classification model from {EFFICIENTNET_PATH}...\")\n",
        "        model = load_model(EFFICIENTNET_PATH)\n",
        "        print(\"Image classification model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image classification model: {e}\")\n",
        "        print(\"Image classification may not work correctly.\")\n",
        "        model = None\n",
        "\n",
        "    # Load fine-tuned LLM model\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned text model from {fine_tuned_dir}...\")\n",
        "\n",
        "        # Load base model first (same config you used during training)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(fine_tuned_dir)\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            fine_tuned_dir,\n",
        "            device_map=\"cpu\",  # or {\"\": \"cpu\"} if you're on CPU\n",
        "            offload_folder=\"./offload\",  # optional if low RAM\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "\n",
        "        # Apply PEFT adapter (if you used QLoRA or PEFT)\n",
        "        model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "\n",
        "        # # Create generation pipeline and assign to tinyllama_llm function\n",
        "        # # llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,\n",
        "        # #                         max_new_tokens=512, do_sample=True)\n",
        "        # llm_pipeline = pipeline(\n",
        "        #           \"text-generation\",\n",
        "        #           model=model,\n",
        "        #           tokenizer=tokenizer,\n",
        "        #           max_new_tokens=512,\n",
        "        #           do_sample=True,\n",
        "        #           temperature=0.7,\n",
        "        #           top_p=0.95\n",
        "        #       )\n",
        "\n",
        "        # Create a wrapper function that matches your existing code's expectations\n",
        "        # def tinyllama_llm(prompt):\n",
        "        #     try:\n",
        "        #         outputs = llm_pipeline(prompt)\n",
        "        #         return outputs[0]['generated_text'].replace(prompt, \"\").strip()\n",
        "        #     except Exception as e:\n",
        "        #         print(f\"Error generating text: {e}\")\n",
        "        #         return \"I apologize, but I'm having trouble generating a response right now.\"\n",
        "\n",
        "        # def tinyllama_llm(prompt):\n",
        "        #   \"\"\"Generate responses using TinyLlama model\"\"\"\n",
        "        #   # print(\"In original function\")\n",
        "        #   try:\n",
        "        #       # Format input for TinyLlama chat model\n",
        "        #       formatted_prompt = f\"<human>: {prompt}\\n<assistant>:\"\n",
        "        #       response = llm_pipeline(formatted_prompt)[0][\"generated_text\"]\n",
        "\n",
        "        #       # Extract only the assistant's response\n",
        "        #       assistant_response = response.split(\"<assistant>:\", 1)[1].strip()\n",
        "\n",
        "        #       # Clean up any potential trailing tokens\n",
        "        #       if \"<human>\" in assistant_response:\n",
        "        #           assistant_response = assistant_response.split(\"<human>\")[0].strip()\n",
        "\n",
        "        #       return assistant_response\n",
        "        #   except Exception as e:\n",
        "        #       print(f\"TinyLlama error: {e}\")\n",
        "        #       return \"I couldn't generate a response. Please try again.\"\n",
        "\n",
        "        def tinyllama_llm(prompt):\n",
        "          \"\"\"Generate responses using TinyLlama model (QLoRA-compatible)\"\"\"\n",
        "          try:\n",
        "              # Format input for TinyLlama chat model\n",
        "              formatted_prompt = f\"<human>: {prompt}\\n<assistant>:\"\n",
        "\n",
        "              # Tokenize the input\n",
        "              inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "              # Generate output\n",
        "              outputs = model.generate(\n",
        "                  **inputs,\n",
        "                  max_new_tokens=512,\n",
        "                  do_sample=True,\n",
        "                  temperature=0.7,\n",
        "                  top_p=0.95,\n",
        "                  pad_token_id=tokenizer.eos_token_id,\n",
        "              )\n",
        "\n",
        "              # Decode and clean response\n",
        "              generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "              assistant_response = generated_text.split(\"<assistant>:\", 1)[1].strip()\n",
        "\n",
        "              if \"<human>\" in assistant_response:\n",
        "                  assistant_response = assistant_response.split(\"<human>\")[0].strip()\n",
        "\n",
        "              return assistant_response\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"TinyLlama error: {e}\")\n",
        "              return \"I couldn't generate a response. Please try again.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Fine-tuned text model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading fine-tuned text model: {e}\")\n",
        "        print(\"Using fallback text generation...\")\n",
        "\n",
        "        # Simple fallback text generation function\n",
        "        def tinyllama_llm(prompt):\n",
        "            return \"I apologize, but my text generation model is not currently available. I can only analyze images at the moment.\"\n",
        "\n",
        "    # Start chat\n",
        "    start_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706,
          "referenced_widgets": [
            "de7f38287cf846708dd8bbf634ac7d8c",
            "c1d17be684bc433fbd24973173633630",
            "75857896b8f64bef963b60b7d54bbc7a",
            "f6793b2c0871448ca7f660916d8b5ee1",
            "2ae6bbde029c4d74b2b481711cb5dc49",
            "40274c9f43ba4ae582133409f03ea826",
            "a4c7a83768844850ad962b246bc44772",
            "6c284bbd473f4e4eb5bad4b7454b2fba",
            "1a10ec9448db4cfc96fd7e6816ddce30",
            "e0281d639bc448c683eff560bff9d770",
            "7c642663dcc3404c905f59c2e1b9f97d",
            "5f38edda5fd247698c0b73c5221b5a55",
            "9c324ecd41304e15b91cafd2ca3bdc08"
          ]
        },
        "id": "r27OBfaBDmzt",
        "outputId": "17b4d280-eaca-4b6b-efa1-f5e61eb5dcd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading image classification model from /content/drive/MyDrive/skin_cancer_model.pth...\n",
            "First few keys in the state dict:\n",
            "  conv_stem.weight\n",
            "  bn1.weight\n",
            "  bn1.bias\n",
            "  bn1.running_mean\n",
            "  bn1.running_var\n",
            "Image classification model loaded successfully!\n",
            "Loading fine-tuned text model from /content/drive/MyDrive/finetuned_model...\n",
            "Fine-tuned text model loaded successfully!\n",
            "Medical Chatbot Ready!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    .chat-container {\n",
              "        max-height: 400px;\n",
              "        overflow-y: auto;\n",
              "        border: 1px solid #ddd;\n",
              "        padding: 10px;\n",
              "        margin-bottom: 10px;\n",
              "        background-color: #f9f9f9;\n",
              "        border-radius: 5px;\n",
              "    }\n",
              "    .user-message {\n",
              "        background-color: #e3f2fd;\n",
              "        padding: 8px 12px;\n",
              "        border-radius: 15px;\n",
              "        margin: 5px 0;\n",
              "        max-width: 80%;\n",
              "        margin-left: auto;\n",
              "        text-align: right;\n",
              "    }\n",
              "    .bot-message {\n",
              "        background-color: #f1f1f1;\n",
              "        padding: 8px 12px;\n",
              "        border-radius: 15px;\n",
              "        margin: 5px 0;\n",
              "        max-width: 80%;\n",
              "    }\n",
              "    .message-container {\n",
              "        display: flex;\n",
              "        flex-direction: column;\n",
              "        margin-bottom: 10px;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(height='400px', overflow_y='auto'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de7f38287cf846708dd8bbf634ac7d8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Text(value='', layout=Layout(width='70%'), placeholder='Type your message here...'), Button(but…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75857896b8f64bef963b60b7d54bbc7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aec91ced-711c-4cec-895d-13372e72b6f0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aec91ced-711c-4cec-895d-13372e72b6f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving melanoma.jpeg to melanoma (1).jpeg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G-9LYwlYDpP",
        "outputId": "a955f8af-48bb-479d-a53e-72302bbe8c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model (if you train it)"
      ],
      "metadata": {
        "id": "H_U6p6H8BnIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION"
      ],
      "metadata": {
        "id": "CH_-N9TOCYHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Evaluation Results:\n",
        "rouge1: 0.2936\n",
        "rouge2: 0.0754\n",
        "rougeL: 0.2391\n",
        "bleu: 0.0271\n",
        "semantic_similarity: 0.7751"
      ],
      "metadata": {
        "id": "MOkaKOOdCb8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TfsP2mcfW6ni",
        "vu1Skz7_XI74",
        "lKd_owQCA9w5"
      ],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 54339,
          "sourceId": 104884,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de7f38287cf846708dd8bbf634ac7d8c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c1d17be684bc433fbd24973173633630",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">Welcome to the Medical Chatbot! How can I help you with your skin condition today?</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">Welcome to the Medical Chatbot! I can provide information about various skin conditions and analyze dermatological images. How can I assist you today?</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"user-message\">red lesions are present</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">Based on the available information, red lesions are typically found on the skin and may be present in people with genetic predispositions to melanoma, such as Ashkenazi Jewish or African American populations. It is recommended to consult a dermatologist for further evaluation and management.</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"user-message\">they are symmetrical in nature</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">Based on the available data, the diagnosis of vascular lesion - abnormal blood vessel formation in the skin is consistent with the symptoms and location described in the given information. The condition is generally benign and does not require medical attention. If you have further questions or concerns, please consult a medical professional.</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">Please select an image of the skin condition...</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"user-message\"><img src=\"data:image/jpeg;base64,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\" style=\"max-width:200px; border-radius:5px;\" /></div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">Analyzing image: melanoma (1).jpeg...</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">Actinic keratosis (AK) is a precancerous skin condition caused by sun damage. It is located on the face, age 60, and sex male. It is a benign growth that can worsen with prolonged exposure to sunlight. The condition should be treated with a sunscreen and frequent sunscreen application. It is essential to seek medical attention if the skin condition persists or worsens.</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"user-message\">h</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">According to the retrieved information, vascular lesion - abnormal blood vessel formation in the skin is a medical condition that affects the skin. This condition is located on the face, has an age range of 15-55 years, and is categorized as histo. It is recommended to seek medical advice from a dermatologist if you have any concerns regarding this condition.</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"user-message\">is it a cancer</div></div>"
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<div class=\"message-container\"><div class=\"bot-message\">Based on the available data, the condition melanoma is a serious form of skin cancer that develops from melanocytes. The condition is located on the neck and has been diagnosed in female patients aged 45-60 years, and male patients aged 60-55 years. It is recommended to seek medical attention if you notice any changes on your skin.</div></div>"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c1d17be684bc433fbd24973173633630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "400px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": "auto",
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75857896b8f64bef963b60b7d54bbc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6793b2c0871448ca7f660916d8b5ee1",
              "IPY_MODEL_2ae6bbde029c4d74b2b481711cb5dc49",
              "IPY_MODEL_40274c9f43ba4ae582133409f03ea826"
            ],
            "layout": "IPY_MODEL_a4c7a83768844850ad962b246bc44772"
          }
        },
        "f6793b2c0871448ca7f660916d8b5ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6c284bbd473f4e4eb5bad4b7454b2fba",
            "placeholder": "Type your message here...",
            "style": "IPY_MODEL_1a10ec9448db4cfc96fd7e6816ddce30",
            "value": ""
          }
        },
        "2ae6bbde029c4d74b2b481711cb5dc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Send",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e0281d639bc448c683eff560bff9d770",
            "style": "IPY_MODEL_7c642663dcc3404c905f59c2e1b9f97d",
            "tooltip": ""
          }
        },
        "40274c9f43ba4ae582133409f03ea826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Upload Image",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5f38edda5fd247698c0b73c5221b5a55",
            "style": "IPY_MODEL_9c324ecd41304e15b91cafd2ca3bdc08",
            "tooltip": ""
          }
        },
        "a4c7a83768844850ad962b246bc44772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c284bbd473f4e4eb5bad4b7454b2fba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "70%"
          }
        },
        "1a10ec9448db4cfc96fd7e6816ddce30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0281d639bc448c683eff560bff9d770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "15%"
          }
        },
        "7c642663dcc3404c905f59c2e1b9f97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5f38edda5fd247698c0b73c5221b5a55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "15%"
          }
        },
        "9c324ecd41304e15b91cafd2ca3bdc08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}